{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BYU BRG CAMDA CMAP Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys, gzip\n",
    "import copy\n",
    "\n",
    "## Import sklearn modules\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, ShuffleSplit, StratifiedKFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "\n",
    "## Import sklearn modules for classifiers\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "## Setting File locations\n",
    "orig_scan_mcf7_train = 'orig_scan/trainingReformatedCamda_MCF7.txt.gz'\n",
    "orig_scan_mcf7_test = 'orig_scan/testReformatedCamda_MCF7.txt.gz'\n",
    "orig_scan_pc3_train = 'orig_scan/trainingReformatedCamda_PC3.txt.gz'\n",
    "orig_scan_pc3_test = 'orig_scan/testReformatedCamda_PC3.txt.gz'\n",
    "scan_mcf7_train = 'scan/scan_mcf7_train.txt.gz'\n",
    "scan_mcf7_test = 'scan/scan_mcf7_test.txt.gz'\n",
    "scan_pc3_train = 'scan/scan_pc3_train.txt.gz'\n",
    "scan_pc3_test = 'scan/scan_pc3_test.txt.gz'\n",
    "farms_mcf7_train = 'farms/farms_mcf7_train.txt.gz'\n",
    "farms_mcf7_test = 'farms/farms_mcf7_test.txt.gz'\n",
    "farms_pc3_train = 'farms/farms_pc3_train.txt.gz'\n",
    "farms_pc3_test = 'farms/farms_pc3_test.txt.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate Accuracy\n",
    "from math import sqrt\n",
    "\n",
    "def getAccuracy(truePositives, trueNegatives, falsePostives, falseNegatives):\n",
    "    return((truePositives + trueNegatives) / float(truePositives + trueNegatives + falseNegatives + falsePostives))\n",
    "\n",
    "def getSensitivity(truePositives, falseNegatives):\n",
    "    return(truePositives/float(truePositives + falseNegatives))\n",
    "\n",
    "def getSpecificity(trueNegatives, falsePostives):\n",
    "    return(trueNegatives/float(trueNegatives + falsePostives))\n",
    "\n",
    "def getMCC(truePositives, trueNegatives, falsePostives, falseNegatives):\n",
    "    return((truePositives * trueNegatives - falsePostives * falseNegatives) / sqrt((truePositives + falsePostives) * (truePositives + falseNegatives) * (trueNegatives + falsePostives) * (trueNegatives + falseNegatives)))\n",
    "\n",
    "def printConfusionCalculations(TP, TN, FP, FN) : \n",
    "    print(\"accuracy: \" + str(getAccuracy(TP, TN, FP, FN)))\n",
    "    print(\"sensitivity: \" + str(getSensitivity(TP, FN)))\n",
    "    print(\"specificity: \" + str(getSpecificity(TN, FP)))\n",
    "    print(\"MCC: \" + str(getMCC(TP, TN, FP, FN)))\n",
    "\n",
    "def getConfusionInformation(TP, TN, FP, FN) :\n",
    "    return [getAccuracy(TP, TN, FP, FN), getSensitivity(TP, FN), getSpecificity(TN, FP), getMCC(TP, TN, FP, FN)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(X_train, X_test, y_train, classifier) :\n",
    "    scaler = StandardScaler()\n",
    "    classifier = make_classifier(classifier)\n",
    "    robust_scaler = RobustScaler(quantile_range=(25, 75))\n",
    "    pca = PCA(n_components = 10)\n",
    "    selected_percentile = SelectPercentile(f_classif, percentile=20)\n",
    "        \n",
    "    pipe = Pipeline(steps=[\n",
    "#         ('s_scaler', scaler),\n",
    "#         ('robust_scaler', robust_scaler),\n",
    "#         ('pca', pca),\n",
    "        ('selected_percentile', selected_percentile),\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "    pipe.fit(X_train,y_train)\n",
    "    \n",
    "    predictions = pipe.predict(X_test)\n",
    "    y_prob = None #pipe.predict_proba(X_test)\n",
    "    \n",
    "    return predictions, y_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(y_test_final, predictions_final, y_prob_final):\n",
    "    TN, FP, FN, TP = confusion_matrix(y_test_final,predictions_final).ravel()\n",
    "    matrix = confusion_matrix(y_test_final,predictions_final)\n",
    "\n",
    "    print(\"\\nConfusion Matrix -\",\n",
    "          \"   True Negative = zeros that were calculated correctly\",\n",
    "          \"   False Negative = zeros that were calculated incorrectly\",\n",
    "          \"   True Positive = ones that were calculated correctly\",\n",
    "          \"   False Positive = ones that were calculated incorrectly\",\n",
    "          \"\\n[[True Negative,False Negative]\",\n",
    "          \"[False Positive,True Positive]]\\n\",\n",
    "          matrix,\n",
    "          \"\\n\",\n",
    "          classification_report(y_test_final,predictions_final),\n",
    "          sep='\\n')\n",
    "    printConfusionCalculations(TP, TN, FP, FN),\n",
    "\n",
    "def get_Results(y_test, predictions):\n",
    "    TN, FP, FN, TP = confusion_matrix(y_test,predictions).ravel()\n",
    "    return getConfusionInformation(TP, TN, FP, FN)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test/Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainFile, classifier):\n",
    "    with gzip.open(trainFile, 'r') as file :\n",
    "        data = np.genfromtxt(file, delimiter='\\t',dtype=str)\n",
    "\n",
    "    ## Split the data up into features and answers\n",
    "    answers = []\n",
    "    features = []\n",
    "    for row in data[1:,]:\n",
    "        answers.append(row[1])\n",
    "        features.append(row[2:])\n",
    "\n",
    "    ## Convert to numpy arrays for algorithms\n",
    "    features = np.array(features,dtype=float)\n",
    "    answers = np.array(answers,dtype=float)\n",
    "    \n",
    "    ## Initialize prediction arrays\n",
    "    y_test_final = np.array([])\n",
    "    predictions_final = np.array([])\n",
    "    y_prob_final = np.ndarray(shape=(0,2), dtype=int)\n",
    "\n",
    "    ## We are using stradified fold cross validation.\n",
    "    skf = StratifiedKFold(n_splits=10)\n",
    "    i = 0\n",
    "\n",
    "    ## Feature Selection needs to happen on each fold independently\n",
    "    for train, test in skf.split(features, answers) :\n",
    "        i += 1\n",
    "        X_train, X_test, y_train, y_test = features[train], features[test], answers[train], answers[test]\n",
    "\n",
    "        predictions, y_prob = make_predictions(X_train,X_test,y_train, classifier)\n",
    "\n",
    "        y_test_final = np.concatenate([y_test_final,y_test])\n",
    "        predictions_final = np.concatenate([predictions_final,predictions])\n",
    "#         y_prob_final = np.concatenate([y_prob_final,y_prob])\n",
    "\n",
    "    return y_test_final, predictions_final, y_prob_final\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(trainFile, testFile, classifier):\n",
    "    with gzip.open(trainFile, 'r') as file :\n",
    "        trainData = np.genfromtxt(file, delimiter='\\t',dtype=str)\n",
    "    with gzip.open(testFile, 'r') as file :\n",
    "        testData = np.genfromtxt(file, delimiter='\\t',dtype=str)\n",
    "\n",
    "    ## training data\n",
    "    y_train = []\n",
    "    X_train = []\n",
    "    for row in trainData[1:,]:\n",
    "        y_train.append(row[1])\n",
    "        X_train.append(row[2:])\n",
    "\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    for row in testData[1:,]:\n",
    "        y_test.append(row[1])\n",
    "        X_test.append(row[2:])\n",
    "\n",
    "    y_train = np.array(y_train,dtype=float)\n",
    "    y_test = np.array(y_test,dtype=float)\n",
    "    X_train = np.array(X_train,dtype=float)\n",
    "    X_test = np.array(X_test,dtype=float)\n",
    "\n",
    "    ## Convert to numpy arrays for algorithms\n",
    "    X_test = np.array(X_test,dtype=float)\n",
    "    X_train = np.array(X_train,dtype=float)\n",
    "\n",
    "    predictions, y_prob = make_predictions(X_train,X_test,y_train, classifier)\n",
    "\n",
    "    return y_test, predictions, y_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _svm():\n",
    "#    return svm.SVC(probability=True) #rbf #kernel='linear', probability=True, class_weight=\"balanced\"\n",
    "    return svm.SVC(kernel='linear', probability=True, class_weight={0:1,1:2})\n",
    "#     return svm.NuSVC(kernel='linear', probability=True, class_weight=\"balanced\") #rbf\n",
    "    \n",
    "def _rf():\n",
    "    return RandomForestClassifier(n_estimators=25,\n",
    "                                max_depth=9,\n",
    "                                min_samples_split=2,\n",
    "                                min_samples_leaf=1,\n",
    "                                min_weight_fraction_leaf=0,\n",
    "                                max_leaf_nodes=25,\n",
    "                                bootstrap=False,\n",
    "                                random_state=0,\n",
    "                                class_weight={0:1,1:2})\n",
    "#                                class_weight=\"balanced\"\n",
    "\n",
    "\n",
    "def _nb():\n",
    "#     return BernoulliNB()\n",
    "    return GaussianNB()\n",
    "\n",
    "def _mlp():\n",
    "#     return MLPClassifier(hidden_layer_sizes=(150,150,150, 150, 150), learning_rate_init = .01)\n",
    "#     return MLPClassifier(hidden_layer_sizes=(200,200,200), learning_rate_init = .01)\n",
    "#    return MLPClassifier(hidden_layer_sizes=(100,100,100), learning_rate_init = .01)\n",
    "#    return MLPClassifier(hidden_layer_sizes=(90,80,70,60), learning_rate_init = .01)\n",
    "#     return MLPClassifier(hidden_layer_sizes=(100,70,60,30,60,70,100), learning_rate_init = .002)\n",
    "#     return MLPClassifier(hidden_layer_sizes=(100,70,60,30), learning_rate_init = .002)\n",
    "#     return MLPClassifier(hidden_layer_sizes=(150,100,50,30), learning_rate_init = .001)\n",
    "#     return MLPClassifier(hidden_layer_sizes=(150,120,90,60,30), learning_rate_init = .001)\n",
    "#     return MLPClassifier(hidden_layer_sizes=(150,130,110,90,70,50), learning_rate_init = .001)\n",
    "#     return MLPClassifier(hidden_layer_sizes=(150,130,110,90,75,60,40), learning_rate_init = .001)\n",
    "#     return MLPClassifier(hidden_layer_sizes=(250,200,150,130,110,90), learning_rate_init = .0003)\n",
    "#     return MLPClassifier(hidden_layer_sizes=(300,250,200,170,140,110,90,50), learning_rate_init = .0003)\n",
    "#     return MLPClassifier(hidden_layer_sizes=(500,400,300,250,200,160,120,90,50), learning_rate_init = .0006)\n",
    "#    return MLPClassifier(hidden_layer_sizes=(80,80,80), learning_rate_init = .01)\n",
    "    return MLPClassifier(hidden_layer_sizes=(30,30,30,30,30,30,30,30,30,30), learning_rate_init = .0376)\n",
    "\n",
    "def _lr():\n",
    "    return linear_model.LogisticRegression(solver='lbfgs', class_weight={0:1,1:2})\n",
    "\n",
    "def _knn():\n",
    "    return KNeighborsClassifier(n_neighbors=8, weights='distance')\n",
    "\n",
    "def _gb():\n",
    "    return GradientBoostingClassifier(learning_rate = .31, max_depth = 3)\n",
    "\n",
    "def _ensemble():\n",
    "#     estimators = [\n",
    "#         ('LR', _lr()),\n",
    "#         ('SVM', _svm()),\n",
    "#         ('KNN', _knn()),\n",
    "#         ('MLP', _mlp()),\n",
    "#         ('RF', _rf()),\n",
    "#         ('GB', _gb()),\n",
    "# #         ('NB', _nb())\n",
    "#     ]\n",
    "    estimators = scaled_ensemble()\n",
    "    \n",
    "    return VotingClassifier(estimators, voting='hard')\n",
    "\n",
    "def make_classifier(classifier) :\n",
    "    if classifier == 'mlp':\n",
    "        return _mlp()\n",
    "    elif classifier == 'rf':\n",
    "        return _rf()\n",
    "    elif classifier == 'nb':\n",
    "        return _nb()\n",
    "    elif classifier == 'knn':\n",
    "        return _knn()\n",
    "    elif classifier == 'svm':\n",
    "        return _svm()\n",
    "    elif classifier == 'lr':\n",
    "        return _lr()\n",
    "    elif classifier == 'gb':\n",
    "        return _gb()\n",
    "    elif classifier == 'ensemble':\n",
    "        return _ensemble()\n",
    "    else:\n",
    "        raise ValueError('Not a correct key for classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_ensemble():\n",
    "    return [\n",
    "        ('MLP_500', MLPClassifier(hidden_layer_sizes=(500,400,300,250,200,160,120,90,50), learning_rate_init = .0006)),\n",
    "        ('MLP_300', MLPClassifier(hidden_layer_sizes=(300,250,200,170,140,110,90,50), learning_rate_init = .0003)),\n",
    "        ('MLP_250', MLPClassifier(hidden_layer_sizes=(250,200,150,130,110,90), learning_rate_init = .0003)),\n",
    "        ('MLP_150', MLPClassifier(hidden_layer_sizes=(150,100,50,30), learning_rate_init = .001)),\n",
    "        ('MLP_30', MLPClassifier(hidden_layer_sizes=(30,30,30,30,30,30,30,30,30,30), learning_rate_init = .0376)),\n",
    "        ('svm_orig', svm.SVC(probability=True)),\n",
    "        ('svm_linear', svm.SVC(kernel='linear', probability=True, class_weight={0:25,1:1})),\n",
    "        ('svm_nusvc_linear', svm.NuSVC(kernel='linear', probability=True, class_weight=\"balanced\")),\n",
    "        ('svm_nusvc_rbf', svm.NuSVC(kernel='rbf', probability=True, class_weight=\"balanced\")),\n",
    "        ('logreg_25_1', linear_model.LogisticRegression(solver='lbfgs', class_weight={0:25,1:1})),\n",
    "        ('logreg_blcd', linear_model.LogisticRegression(solver='lbfgs', class_weight=\"balanced\")),\n",
    "        ('rf_norm', RandomForestClassifier(n_estimators=25,\n",
    "                                max_depth=9,\n",
    "                                min_samples_split=2,\n",
    "                                min_samples_leaf=1,\n",
    "                                min_weight_fraction_leaf=0,\n",
    "                                max_leaf_nodes=25,\n",
    "                                bootstrap=False,\n",
    "                                random_state=0)),\n",
    "        ('rf_blcd_25_9', RandomForestClassifier(n_estimators=25,\n",
    "                                max_depth=9,\n",
    "                                min_samples_split=2,\n",
    "                                min_samples_leaf=1,\n",
    "                                min_weight_fraction_leaf=0,\n",
    "                                max_leaf_nodes=25,\n",
    "                                bootstrap=False,\n",
    "                                random_state=0)),\n",
    "        ('rf_blcd_100_9', RandomForestClassifier(n_estimators=100,\n",
    "                                max_depth=9,\n",
    "                                min_samples_split=2,\n",
    "                                min_samples_leaf=1,\n",
    "                                min_weight_fraction_leaf=0,\n",
    "                                max_leaf_nodes=25,\n",
    "                                bootstrap=False,\n",
    "                                random_state=0,\n",
    "                                class_weight=\"balanced\")),\n",
    "        ('rf_blcd_100_15', RandomForestClassifier(n_estimators=100,\n",
    "                                max_depth=15,\n",
    "                                min_samples_split=2,\n",
    "                                min_samples_leaf=1,\n",
    "                                min_weight_fraction_leaf=0,\n",
    "                                max_leaf_nodes=25,\n",
    "                                bootstrap=False,\n",
    "                                random_state=0,\n",
    "                                class_weight=\"balanced\")),\n",
    "        ('rf_blcd_150_15', RandomForestClassifier(n_estimators=50,\n",
    "                                max_depth=15,\n",
    "                                min_samples_split=2,\n",
    "                                min_samples_leaf=1,\n",
    "                                min_weight_fraction_leaf=0,\n",
    "                                max_leaf_nodes=25,\n",
    "                                bootstrap=False,\n",
    "                                random_state=0,\n",
    "                                class_weight=\"balanced\")),\n",
    "        ('knn_8', KNeighborsClassifier(n_neighbors=8, weights='distance')),\n",
    "        ('knn_12', KNeighborsClassifier(n_neighbors=12, weights='distance')),\n",
    "        ('knn_10', KNeighborsClassifier(n_neighbors=10, weights='distance')),\n",
    "        ('gb_31_3', GradientBoostingClassifier(learning_rate = .31, max_depth = 3)),\n",
    "        ('gb_15_5', GradientBoostingClassifier(learning_rate = .15, max_depth = 5)),\n",
    "        ('gb_07_6', GradientBoostingClassifier(learning_rate = .07, max_depth = 6))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Testing scan/scan_pc3_test.txt.gz with ensemble\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2: Testing scan/scan_mcf7_test.txt.gz with ensemble\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Test/Train</th>\n",
       "      <th>Cell Line</th>\n",
       "      <th>Data_Version</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ensemble</td>\n",
       "      <td>Test</td>\n",
       "      <td>PC3</td>\n",
       "      <td>updated_scan</td>\n",
       "      <td>0.697674</td>\n",
       "      <td>0.880597</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>-0.090498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ensemble</td>\n",
       "      <td>Test</td>\n",
       "      <td>MCF7</td>\n",
       "      <td>updated_scan</td>\n",
       "      <td>0.709302</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.068857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Method Test/Train Cell Line  Data_Version  Accuracy  Sensitivity  \\\n",
       "1  ensemble       Test       PC3  updated_scan  0.697674     0.880597   \n",
       "2  ensemble       Test      MCF7  updated_scan  0.709302     0.850746   \n",
       "\n",
       "   Specificity       MCC  \n",
       "1     0.052632 -0.090498  \n",
       "2     0.210526  0.068857  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifiers = [\n",
    "#     'rf',\n",
    "#     'svm',\n",
    "#     'nb',\n",
    "#     'mlp',\n",
    "#     'knn',\n",
    "#     'lr',\n",
    "#     'gb',\n",
    "    'ensemble'\n",
    "]\n",
    "train_files = [\n",
    "#     [orig_scan_pc3_train, 'Training', 'PC3', 'orig_scan'],\n",
    "#     [orig_scan_mcf7_train, 'Training', 'MCF7', 'orig_scan'],\n",
    "#     [scan_pc3_train, 'Training', 'PC3', 'updated_scan'],\n",
    "#     [scan_mcf7_train, 'Training', 'MCF7', 'updated_scan'],\n",
    "#     [farms_pc3_train, 'Training', 'PC3', 'farms'],\n",
    "#     [farms_mcf7_train, 'Training', 'MCF7', 'farms']\n",
    "]\n",
    "test_files = [\n",
    "#     [orig_scan_pc3_train, orig_scan_pc3_test, 'Test', 'PC3', 'orig_scan'],\n",
    "#     [orig_scan_mcf7_train, orig_scan_mcf7_test, 'Test', 'MCF7', 'orig_scan'],\n",
    "    [scan_pc3_train, scan_pc3_test, 'Test', 'PC3', 'updated_scan'],\n",
    "    [scan_mcf7_train, scan_mcf7_test, 'Test', 'MCF7', 'updated_scan'],\n",
    "#     [farms_pc3_train, farms_pc3_test, 'Test', 'PC3', 'farms'],\n",
    "#     [farms_mcf7_train, farms_mcf7_test, 'Test', 'MCF7', 'farms'],\n",
    "]\n",
    "\n",
    "col_names =  ['Method', 'Test/Train', 'Cell Line', 'Data_Version', 'Accuracy', 'Sensitivity', 'Specificity', 'MCC']\n",
    "df  = pd.DataFrame(columns = col_names)\n",
    "\n",
    "i = 0\n",
    "for classifier in classifiers:\n",
    "    for file_set in train_files:\n",
    "        i += 1\n",
    "        print(\"Iteration {}: Training {} with {}\".format(i, file_set[0], classifier))\n",
    "        y_test, predictions, _ = train(file_set[0],classifier)\n",
    "        df.loc[i] = [classifier, file_set[1], file_set[2], file_set[3]] + get_Results(y_test, predictions)\n",
    "        \n",
    "for classifier in classifiers:\n",
    "    for file_set in test_files:\n",
    "        i += 1\n",
    "        print(\"Iteration {}: Testing {} with {}\".format(i, file_set[1], classifier))\n",
    "        y_test, predictions, _ = test(file_set[0], file_set[1], classifier)\n",
    "        df.loc[i] = [classifier, file_set[2], file_set[3], file_set[4]] + get_Results(y_test, predictions)\n",
    "        \n",
    "print('done')\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_df = df\n",
    "# original_df.to_csv('original.csv')\n",
    "\n",
    "# updated_svm_df = df\n",
    "# updated_svm_df.to_csv('updated_svm.csv')\n",
    "# original_df\n",
    "\n",
    "# robust_scaler_df = df\n",
    "# robust_scaler_df.to_csv('robust_scaler.csv')\n",
    "\n",
    "# orig_anova_df = df\n",
    "# orig_anova_df.to_csv('orig_anova.csv')\n",
    "\n",
    "# orig_pca_df = df\n",
    "# orig_pca_df.to_csv('orig_pca.csv')\n",
    "\n",
    "# balanced_df = df\n",
    "# balanced_df.to_csv('balanced.csv')\n",
    "\n",
    "# weight_1_50_df = df\n",
    "# weight_1_50_df.to_csv('weight_1_50.csv')\n",
    "\n",
    "# weight_50_1_df = df\n",
    "# weight_50_1_df.to_csv('weight_50_1.csv')\n",
    "\n",
    "# weight_25_1_df = df\n",
    "# weight_25_1_df.to_csv('weight_25_1.csv')\n",
    "\n",
    "# weight_10_1_df = df\n",
    "# weight_10_1_df.to_csv('weight_10_1.csv')\n",
    "\n",
    "# weight_5_1_df = df\n",
    "# weight_5_1_df.to_csv('weight_5_1.csv')\n",
    "\n",
    "# weight_2_1_df = df\n",
    "# weight_2_1_df.to_csv('weight_2_1.csv')\n",
    "\n",
    "# weight_1_2_df = df\n",
    "# weight_1_2_df.to_csv('weight_1_2.csv')\n",
    "\n",
    "# scaled_ensemble_df = df\n",
    "# scaled_ensemble_df.to_csv('scaled_ensemble.csv')\n",
    "\n",
    "# scaled_ensemble_hard_df = df\n",
    "# scaled_ensemble_hard_df.to_csv('scaled_ensemble_hard.csv')\n",
    "\n",
    "# norm_ensemble_hard_df = df\n",
    "# norm_ensemble_hard_df.to_csv('norm_ensemble_hard.csv')\n",
    "\n",
    "best_df = df\n",
    "best_df.to_csv('best.csv')\n",
    "\n",
    "original_df = pd.read_csv('original.csv')\n",
    "updated_svm_df = pd.read_csv('updated_svm.csv')\n",
    "robust_scaler_df = pd.read_csv('robust_scaler.csv')\n",
    "orig_anova_df = pd.read_csv('orig_anova.csv')\n",
    "orig_pca_df = pd.read_csv('orig_pca.csv')\n",
    "\n",
    "weight_1_50_df = pd.read_csv('weight_1_50.csv')\n",
    "balanced_df = pd.read_csv('balanced.csv')\n",
    "weight_50_1_df = pd.read_csv('weight_50_1.csv')\n",
    "weight_25_1_df = pd.read_csv('weight_25_1.csv')\n",
    "weight_10_1_df = pd.read_csv('weight_10_1.csv')\n",
    "weight_5_1_df = pd.read_csv('weight_5_1.csv')\n",
    "weight_2_1_df = pd.read_csv('weight_2_1.csv')\n",
    "weight_1_2_df = pd.read_csv('weight_1_2.csv')\n",
    "\n",
    "scaled_ensemble_df = pd.read_csv('scaled_ensemble.csv')\n",
    "scaled_ensemble_hard_df = pd.read_csv('scaled_ensemble_hard.csv')\n",
    "norm_ensemble_hard_df = pd.read_csv('norm_ensemble_hard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "origClassifiers_farms = original_df[original_df['Data_Version'] == 'farms']\n",
    "origClassifiers_orig_scan = original_df[original_df['Data_Version'] == 'orig_scan']\n",
    "origClassifiers_updated_scan = original_df[original_df['Data_Version'] == 'updated_scan']\n",
    "\n",
    "limited_orig_scan = origClassifiers_orig_scan[origClassifiers_orig_scan['Test/Train'] == 'Test']\n",
    "limited_orig_scan = limited_orig_scan.loc[:,['Accuracy', 'Sensitivity', 'Specificity', 'MCC']]\n",
    "limited_orig_scan.index = range(1,17)\n",
    "\n",
    "limited_farms = origClassifiers_farms[origClassifiers_farms['Test/Train'] == 'Test']\n",
    "limited_farms = limited_farms.loc[:,['Accuracy', 'Sensitivity', 'Specificity', 'MCC']]\n",
    "limited_farms.index = range(1,17)\n",
    "\n",
    "limited_updated_scan = origClassifiers_updated_scan[origClassifiers_updated_scan['Test/Train'] == 'Test']\n",
    "limited_updated_scan = limited_updated_scan.loc[:,['Accuracy', 'Sensitivity', 'Specificity', 'MCC']]\n",
    "limited_updated_scan.index = range(1,17)\n",
    "\n",
    "limited_robust_scaler_df = robust_scaler_df[robust_scaler_df['Test/Train'] == 'Test']\n",
    "limited_robust_scaler_df = limited_robust_scaler_df.loc[:,['Accuracy', 'Sensitivity', 'Specificity', 'MCC']]\n",
    "limited_robust_scaler_df.index = range(1,17)\n",
    "\n",
    "limited_orig_anova_df = orig_anova_df[orig_anova_df['Test/Train'] == 'Test']\n",
    "limited_orig_anova_df = limited_orig_anova_df.loc[:,['Accuracy', 'Sensitivity', 'Specificity', 'MCC']]\n",
    "limited_orig_anova_df.index = range(1,17)\n",
    "\n",
    "limited_orig_pca_df = orig_pca_df[orig_pca_df['Test/Train'] == 'Test']\n",
    "limited_orig_pca_df = limited_orig_pca_df.loc[:,['Accuracy', 'Sensitivity', 'Specificity', 'MCC']]\n",
    "limited_orig_pca_df.index = range(1,17)\n",
    "\n",
    "limited_balanced_df = balanced_df[balanced_df['Test/Train'] == 'Test']\n",
    "limited_balanced_df = limited_balanced_df.loc[:,['Accuracy', 'Sensitivity', 'Specificity', 'MCC']]\n",
    "limited_balanced_df.index = range(1,17)\n",
    "\n",
    "limited_weight_1_50_df = weight_1_50_df[weight_1_50_df['Test/Train'] == 'Test']\n",
    "limited_weight_1_50_df = limited_weight_1_50_df[limited_weight_1_50_df['Method'].isin(['rf', 'svm', 'lr', 'ensemble'])]\n",
    "limited_weight_1_50_df = limited_weight_1_50_df.loc[:,['Accuracy', 'Sensitivity', 'Specificity', 'MCC']]\n",
    "limited_weight_1_50_df.index = range(1,9)\n",
    "\n",
    "limited_weight_50_1_df = weight_50_1_df[weight_50_1_df['Test/Train'] == 'Test']\n",
    "limited_weight_50_1_df = limited_weight_50_1_df[limited_weight_50_1_df['Method'].isin(['rf', 'svm', 'lr', 'ensemble'])]\n",
    "limited_weight_50_1_df = limited_weight_50_1_df.loc[:,['Accuracy', 'Sensitivity', 'Specificity', 'MCC']]\n",
    "limited_weight_50_1_df.index = range(1,9)\n",
    "\n",
    "limited_weight_25_1_df = weight_25_1_df[weight_25_1_df['Test/Train'] == 'Test']\n",
    "limited_weight_25_1_df = limited_weight_25_1_df[limited_weight_25_1_df['Method'].isin(['rf', 'svm', 'lr', 'ensemble'])]\n",
    "limited_weight_25_1_df = limited_weight_25_1_df.loc[:,['Accuracy', 'Sensitivity', 'Specificity', 'MCC']]\n",
    "limited_weight_25_1_df.index = range(1,9)\n",
    "\n",
    "limited_weight_10_1_df = weight_10_1_df[weight_10_1_df['Test/Train'] == 'Test']\n",
    "limited_weight_10_1_df = limited_weight_10_1_df[limited_weight_10_1_df['Method'].isin(['rf', 'svm', 'lr', 'ensemble'])]\n",
    "limited_weight_10_1_df = limited_weight_10_1_df.loc[:,['Accuracy', 'Sensitivity', 'Specificity', 'MCC']]\n",
    "limited_weight_10_1_df.index = range(1,9)\n",
    "\n",
    "limited_weight_5_1_df = weight_5_1_df[weight_5_1_df['Test/Train'] == 'Test']\n",
    "limited_weight_5_1_df = limited_weight_5_1_df[limited_weight_5_1_df['Method'].isin(['rf', 'svm', 'lr', 'ensemble'])]\n",
    "limited_weight_5_1_df = limited_weight_5_1_df.loc[:,['Accuracy', 'Sensitivity', 'Specificity', 'MCC']]\n",
    "limited_weight_5_1_df.index = range(1,9)\n",
    "\n",
    "limited_weight_2_1_df = weight_2_1_df[weight_2_1_df['Test/Train'] == 'Test']\n",
    "limited_weight_2_1_df = limited_weight_2_1_df[limited_weight_2_1_df['Method'].isin(['rf', 'svm', 'lr', 'ensemble'])]\n",
    "limited_weight_2_1_df = limited_weight_2_1_df.loc[:,['Accuracy', 'Sensitivity', 'Specificity', 'MCC']]\n",
    "limited_weight_2_1_df.index = range(1,9)\n",
    "\n",
    "limited_weight_1_2_df = weight_1_2_df[weight_1_2_df['Test/Train'] == 'Test']\n",
    "limited_weight_1_2_df = limited_weight_1_2_df[limited_weight_1_2_df['Method'].isin(['rf', 'svm', 'lr', 'ensemble'])]\n",
    "limited_weight_1_2_df = limited_weight_1_2_df.loc[:,['Accuracy', 'Sensitivity', 'Specificity', 'MCC']]\n",
    "limited_weight_1_2_df.index = range(1,9)\n",
    "\n",
    "limited_scaled_ensemble_df = scaled_ensemble_df[scaled_ensemble_df['Test/Train'] == 'Test']\n",
    "limited_scaled_ensemble_df = limited_scaled_ensemble_df.loc[:,['Accuracy', 'Sensitivity', 'Specificity', 'MCC']]\n",
    "limited_scaled_ensemble_df.index = range(1,3)\n",
    "\n",
    "limited_scaled_ensemble_hard_df = scaled_ensemble_hard_df[scaled_ensemble_hard_df['Test/Train'] == 'Test']\n",
    "limited_scaled_ensemble_hard_df = limited_scaled_ensemble_hard_df.loc[:,['Accuracy', 'Sensitivity', 'Specificity', 'MCC']]\n",
    "limited_scaled_ensemble_hard_df.index = range(1,3)\n",
    "\n",
    "limited_norm_ensemble_hard_df = norm_ensemble_hard_df[norm_ensemble_hard_df['Test/Train'] == 'Test']\n",
    "limited_norm_ensemble_hard_df = limited_norm_ensemble_hard_df.loc[:,['Accuracy', 'Sensitivity', 'Specificity', 'MCC']]\n",
    "limited_norm_ensemble_hard_df.index = range(1,3)\n",
    "\n",
    "limited_best_df = best_df.loc[:,['Accuracy', 'Sensitivity', 'Specificity', 'MCC']]\n",
    "limited_best_df.index = range(1,3)\n",
    "\n",
    "# limited_df = df.loc[:,['Accuracy', 'Sensitivity', 'Specificity', 'MCC']]\n",
    "# limited_df.index = range(1,33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.034884</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.159631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.011628</td>\n",
       "      <td>-0.029851</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.022689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.011628</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.052632</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.069767</td>\n",
       "      <td>-0.089552</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.073541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.164179</td>\n",
       "      <td>-0.368421</td>\n",
       "      <td>-0.173779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.558140</td>\n",
       "      <td>0.970149</td>\n",
       "      <td>-0.894737</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.093023</td>\n",
       "      <td>-0.149254</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.011628</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.058140</td>\n",
       "      <td>0.074627</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.034884</td>\n",
       "      <td>-0.044776</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.040588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.023256</td>\n",
       "      <td>-0.044776</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>-0.000608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.023256</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.080887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.059701</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.087709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.034884</td>\n",
       "      <td>-0.044776</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.116328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.034884</td>\n",
       "      <td>0.044776</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.116328</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Accuracy  Sensitivity  Specificity       MCC\n",
       "1   0.034884     0.014925     0.105263  0.159631\n",
       "2  -0.011628    -0.029851     0.052632  0.022689\n",
       "3  -0.011628     0.000000    -0.052632       NaN\n",
       "4   0.000000     0.000000     0.000000       NaN\n",
       "5  -0.069767    -0.089552     0.000000 -0.073541\n",
       "6   0.046512     0.164179    -0.368421 -0.173779\n",
       "7   0.558140     0.970149    -0.894737       NaN\n",
       "8  -0.093023    -0.149254     0.105263       NaN\n",
       "9   0.011628     0.014925     0.000000  0.016446\n",
       "10  0.058140     0.074627     0.000000  0.086650\n",
       "11 -0.034884    -0.044776     0.000000 -0.040588\n",
       "12 -0.023256    -0.044776     0.052632 -0.000608\n",
       "13  0.023256     0.014925     0.052632  0.080887\n",
       "14  0.046512     0.059701     0.000000  0.087709\n",
       "15 -0.034884    -0.044776     0.000000 -0.116328\n",
       "16  0.034884     0.044776     0.000000  0.116328"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_names =  ['Accuracy', 'Sensitivity', 'Specificity', 'MCC']\n",
    "data_preprocessing  = pd.DataFrame(columns = col_names)\n",
    "\n",
    "data_preprocessing.loc['farms'] = limited_farms.sub(limited_orig_scan).mean()\n",
    "data_preprocessing.loc['updated_scan'] = limited_updated_scan.sub(limited_orig_scan).mean()\n",
    "\n",
    "data_preprocessing\n",
    "\n",
    "# limited_farms.sub(limited_orig_scan)\n",
    "# limited_updated_scan.sub(limited_orig_scan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>robust_scaler</th>\n",
       "      <td>-0.004360</td>\n",
       "      <td>-0.008396</td>\n",
       "      <td>0.009868</td>\n",
       "      <td>-0.005128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anova_f_stat</th>\n",
       "      <td>-0.015988</td>\n",
       "      <td>-0.032649</td>\n",
       "      <td>0.042763</td>\n",
       "      <td>0.016525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pca</th>\n",
       "      <td>0.013081</td>\n",
       "      <td>0.024254</td>\n",
       "      <td>-0.026316</td>\n",
       "      <td>-0.019417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Accuracy  Sensitivity  Specificity       MCC\n",
       "robust_scaler -0.004360    -0.008396     0.009868 -0.005128\n",
       "anova_f_stat  -0.015988    -0.032649     0.042763  0.016525\n",
       "pca            0.013081     0.024254    -0.026316 -0.019417"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_names =  ['Accuracy', 'Sensitivity', 'Specificity', 'MCC']\n",
    "feature_selection  = pd.DataFrame(columns = col_names)\n",
    "\n",
    "feature_selection.loc['robust_scaler'] = limited_robust_scaler_df.sub(limited_orig_scan).mean()\n",
    "feature_selection.loc['anova_f_stat'] = limited_orig_anova_df.sub(limited_orig_scan).mean()\n",
    "feature_selection.loc['pca'] = limited_orig_pca_df.sub(limited_orig_scan).mean()\n",
    "\n",
    "feature_selection\n",
    "\n",
    "# limited_robust_scaler_df.sub(limited_orig_scan)\n",
    "# limited_orig_anova_df.sub(limited_orig_scan)\n",
    "# limited_orig_pca_df.sub(limited_orig_scan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accuracy       0.546512\n",
       "Sensitivity    0.985075\n",
       "Specificity    0.368421\n",
       "MCC            0.054678\n",
       "dtype: float64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_names =  ['Accuracy', 'Sensitivity', 'Specificity', 'MCC']\n",
    "class_weights  = pd.DataFrame(columns = col_names)\n",
    "\n",
    "class_weights.loc['weight_50_1'] = limited_weight_50_1_df.sub(limited_orig_scan).mean()\n",
    "class_weights.loc['weight_25_1'] = limited_weight_25_1_df.sub(limited_orig_scan).mean()\n",
    "class_weights.loc['weight_10_1'] = limited_weight_10_1_df.sub(limited_orig_scan).mean()\n",
    "class_weights.loc['weight_5_1'] = limited_weight_5_1_df.sub(limited_orig_scan).mean()\n",
    "class_weights.loc['weight_2_1'] = limited_weight_2_1_df.sub(limited_orig_scan).mean()\n",
    "class_weights.loc['balanced'] = limited_balanced_df.sub(limited_orig_scan).mean()\n",
    "class_weights.loc['weight_1_2'] = limited_weight_1_2_df.sub(limited_orig_scan).mean()\n",
    "\n",
    "class_weights\n",
    "\n",
    "# limited_weight_50_1_df.sub(limited_orig_scan).min()\n",
    "# limited_weight_25_1_df.sub(limited_orig_scan).max()\n",
    "# limited_weight_10_1_df.sub(limited_orig_scan).max()\n",
    "# limited_weight_5_1_df.sub(limited_orig_scan).max()\n",
    "# limited_weight_2_1_df.sub(limited_orig_scan).max()\n",
    "# limited_balanced_df.sub(limited_orig_scan).max()\n",
    "# limited_weight_1_2_df.sub(limited_orig_scan).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>scaled_soft</th>\n",
       "      <td>-0.029070</td>\n",
       "      <td>-0.082090</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.055568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scaled_hard</th>\n",
       "      <td>-0.046512</td>\n",
       "      <td>-0.089552</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>-0.010646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>norm_hard</th>\n",
       "      <td>-0.034884</td>\n",
       "      <td>-0.059701</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>-0.037789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best</th>\n",
       "      <td>-0.058140</td>\n",
       "      <td>-0.097015</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>-0.056450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Accuracy  Sensitivity  Specificity       MCC\n",
       "scaled_soft -0.029070    -0.082090     0.157895  0.055568\n",
       "scaled_hard -0.046512    -0.089552     0.105263 -0.010646\n",
       "norm_hard   -0.034884    -0.059701     0.052632 -0.037789\n",
       "best        -0.058140    -0.097015     0.078947 -0.056450"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limited_ensemble_orig_scan = limited_orig_scan.loc[15:16]\n",
    "limited_ensemble_orig_scan.index = range(1,3)\n",
    "\n",
    "col_names =  ['Accuracy', 'Sensitivity', 'Specificity', 'MCC']\n",
    "class_weights  = pd.DataFrame(columns = col_names)\n",
    "\n",
    "class_weights.loc['scaled_soft'] = limited_scaled_ensemble_df.sub(limited_ensemble_orig_scan).mean()\n",
    "class_weights.loc['scaled_hard'] = limited_scaled_ensemble_hard_df.sub(limited_ensemble_orig_scan).mean()\n",
    "class_weights.loc['norm_hard'] = limited_norm_ensemble_hard_df.sub(limited_ensemble_orig_scan).mean()\n",
    "# class_weights.loc['best'] = limited_best_df.sub(limited_ensemble_orig_scan).mean()\n",
    "\n",
    "\n",
    "class_weights\n",
    "\n",
    "# limited_norm_ensemble_hard_df.sub(limited_ensemble_orig_scan)\n",
    "# limited_scaled_ensemble_df.sub(limited_ensemble_orig_scan)\n",
    "# limited_scaled_ensemble_hard_df.sub(limited_ensemble_orig_scan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp = tmp_orig_pca_df.sub(tmp_orig_scan).mean()\n",
    "# tmp_df = tmp_orig_pca_df ## pca 10\n",
    "# tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
