{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BYU BRG CAMDA CMAP Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys, gzip\n",
    "import copy\n",
    "\n",
    "## Import sklearn modules\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, ShuffleSplit, StratifiedKFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "\n",
    "## Import sklearn modules for classifiers\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "## Setting File locations\n",
    "orig_scan_mcf7_train = 'orig_scan/trainingReformatedCamda_MCF7.txt.gz'\n",
    "orig_scan_mcf7_test = 'orig_scan/testReformatedCamda_MCF7.txt.gz'\n",
    "orig_scan_pc3_train = 'orig_scan/trainingReformatedCamda_PC3.txt.gz'\n",
    "orig_scan_pc3_test = 'orig_scan/testReformatedCamda_PC3.txt.gz'\n",
    "scan_mcf7_train = 'scan/scan_mcf7_train.txt.gz'\n",
    "scan_mcf7_test = 'scan/scan_mcf7_test.txt.gz'\n",
    "scan_pc3_train = 'scan/scan_pc3_train.txt.gz'\n",
    "scan_pc3_test = 'scan/scan_pc3_test.txt.gz'\n",
    "farms_mcf7_train = 'farms/farms_mcf7_train.txt.gz'\n",
    "farms_mcf7_test = 'farms/farms_mcf7_test.txt.gz'\n",
    "farms_pc3_train = 'farms/farms_pc3_train.txt.gz'\n",
    "farms_pc3_test = 'farms/farms_pc3_test.txt.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate Accuracy\n",
    "from math import sqrt\n",
    "\n",
    "def getAccuracy(truePositives, trueNegatives, falsePostives, falseNegatives):\n",
    "    return((truePositives + trueNegatives) / float(truePositives + trueNegatives + falseNegatives + falsePostives))\n",
    "\n",
    "def getSensitivity(truePositives, falseNegatives):\n",
    "    return(truePositives/float(truePositives + falseNegatives))\n",
    "\n",
    "def getSpecificity(trueNegatives, falsePostives):\n",
    "    return(trueNegatives/float(trueNegatives + falsePostives))\n",
    "\n",
    "def getMCC(truePositives, trueNegatives, falsePostives, falseNegatives):\n",
    "    return((truePositives * trueNegatives - falsePostives * falseNegatives) / sqrt((truePositives + falsePostives) * (truePositives + falseNegatives) * (trueNegatives + falsePostives) * (trueNegatives + falseNegatives)))\n",
    "\n",
    "def printConfusionCalculations(TP, TN, FP, FN) : \n",
    "    print(\"accuracy: \" + str(getAccuracy(TP, TN, FP, FN)))\n",
    "    print(\"sensitivity: \" + str(getSensitivity(TP, FN)))\n",
    "    print(\"specificity: \" + str(getSpecificity(TN, FP)))\n",
    "    print(\"MCC: \" + str(getMCC(TP, TN, FP, FN)))\n",
    "\n",
    "def getConfusionInformation(TP, TN, FP, FN) :\n",
    "    return [getAccuracy(TP, TN, FP, FN), getSensitivity(TP, FN), getSpecificity(TN, FP), getMCC(TP, TN, FP, FN)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(X_train, X_test, y_train, classifier) :\n",
    "    scaler = StandardScaler()\n",
    "    classifier = make_classifier(classifier)\n",
    "    robust_scaler = RobustScaler(quantile_range=(25, 75))\n",
    "    pca = PCA(n_components = 10)\n",
    "    selected_percentile = SelectPercentile(f_classif, percentile=20)\n",
    "        \n",
    "    pipe = Pipeline(steps=[\n",
    "#         ('s_scaler', scaler),\n",
    "#         ('robust_scaler', robust_scaler),\n",
    "#         ('pca', pca),\n",
    "#         ('selected_percentile', selected_percentile),\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "    pipe.fit(X_train,y_train)\n",
    "    \n",
    "    predictions = pipe.predict(X_test)\n",
    "    y_prob = None #pipe.predict_proba(X_test)\n",
    "    \n",
    "    return predictions, y_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(y_test_final, predictions_final, y_prob_final):\n",
    "    TN, FP, FN, TP = confusion_matrix(y_test_final,predictions_final).ravel()\n",
    "    matrix = confusion_matrix(y_test_final,predictions_final)\n",
    "\n",
    "    print(\"\\nConfusion Matrix -\",\n",
    "          \"   True Negative = zeros that were calculated correctly\",\n",
    "          \"   False Negative = zeros that were calculated incorrectly\",\n",
    "          \"   True Positive = ones that were calculated correctly\",\n",
    "          \"   False Positive = ones that were calculated incorrectly\",\n",
    "          \"\\n[[True Negative,False Negative]\",\n",
    "          \"[False Positive,True Positive]]\\n\",\n",
    "          matrix,\n",
    "          \"\\n\",\n",
    "          classification_report(y_test_final,predictions_final),\n",
    "          sep='\\n')\n",
    "    printConfusionCalculations(TP, TN, FP, FN),\n",
    "\n",
    "def get_Results(y_test, predictions):\n",
    "    TN, FP, FN, TP = confusion_matrix(y_test,predictions).ravel()\n",
    "    return getConfusionInformation(TP, TN, FP, FN)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test/Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainFile, classifier):\n",
    "    with gzip.open(trainFile, 'r') as file :\n",
    "        data = np.genfromtxt(file, delimiter='\\t',dtype=str)\n",
    "\n",
    "    ## Split the data up into features and answers\n",
    "    answers = []\n",
    "    features = []\n",
    "    for row in data[1:,]:\n",
    "        answers.append(row[1])\n",
    "        features.append(row[2:])\n",
    "\n",
    "    ## Convert to numpy arrays for algorithms\n",
    "    features = np.array(features,dtype=float)\n",
    "    answers = np.array(answers,dtype=float)\n",
    "    \n",
    "    ## Initialize prediction arrays\n",
    "    y_test_final = np.array([])\n",
    "    predictions_final = np.array([])\n",
    "    y_prob_final = np.ndarray(shape=(0,2), dtype=int)\n",
    "\n",
    "    ## We are using stradified fold cross validation.\n",
    "    skf = StratifiedKFold(n_splits=10)\n",
    "    i = 0\n",
    "\n",
    "    ## Feature Selection needs to happen on each fold independently\n",
    "    for train, test in skf.split(features, answers) :\n",
    "        i += 1\n",
    "        X_train, X_test, y_train, y_test = features[train], features[test], answers[train], answers[test]\n",
    "\n",
    "        predictions, y_prob = make_predictions(X_train,X_test,y_train, classifier)\n",
    "\n",
    "        y_test_final = np.concatenate([y_test_final,y_test])\n",
    "        predictions_final = np.concatenate([predictions_final,predictions])\n",
    "#         y_prob_final = np.concatenate([y_prob_final,y_prob])\n",
    "\n",
    "    return y_test_final, predictions_final, y_prob_final\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(trainFile, testFile, classifier):\n",
    "    with gzip.open(trainFile, 'r') as file :\n",
    "        trainData = np.genfromtxt(file, delimiter='\\t',dtype=str)\n",
    "    with gzip.open(testFile, 'r') as file :\n",
    "        testData = np.genfromtxt(file, delimiter='\\t',dtype=str)\n",
    "\n",
    "    ## training data\n",
    "    y_train = []\n",
    "    X_train = []\n",
    "    for row in trainData[1:,]:\n",
    "        y_train.append(row[1])\n",
    "        X_train.append(row[2:])\n",
    "\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    for row in testData[1:,]:\n",
    "        y_test.append(row[1])\n",
    "        X_test.append(row[2:])\n",
    "\n",
    "    y_train = np.array(y_train,dtype=float)\n",
    "    y_test = np.array(y_test,dtype=float)\n",
    "    X_train = np.array(X_train,dtype=float)\n",
    "    X_test = np.array(X_test,dtype=float)\n",
    "\n",
    "    ## Convert to numpy arrays for algorithms\n",
    "    X_test = np.array(X_test,dtype=float)\n",
    "    X_train = np.array(X_train,dtype=float)\n",
    "\n",
    "    predictions, y_prob = make_predictions(X_train,X_test,y_train, classifier)\n",
    "\n",
    "    return y_test, predictions, y_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _svm():\n",
    "#    return svm.SVC(probability=True) #rbf #kernel='linear', probability=True, class_weight=\"balanced\"\n",
    "    return svm.SVC(kernel='linear', probability=True, class_weight={0:1,1:2})\n",
    "#     return svm.NuSVC(kernel='linear', probability=True, class_weight=\"balanced\") #rbf\n",
    "    \n",
    "def _rf():\n",
    "    return RandomForestClassifier(n_estimators=25,\n",
    "                                max_depth=9,\n",
    "                                min_samples_split=2,\n",
    "                                min_samples_leaf=1,\n",
    "                                min_weight_fraction_leaf=0,\n",
    "                                max_leaf_nodes=25,\n",
    "                                bootstrap=False,\n",
    "                                random_state=0,\n",
    "                                class_weight={0:1,1:2})\n",
    "#                                class_weight=\"balanced\"\n",
    "\n",
    "\n",
    "def _nb():\n",
    "#     return BernoulliNB()\n",
    "    return GaussianNB()\n",
    "\n",
    "def _mlp():\n",
    "#     return MLPClassifier(hidden_layer_sizes=(150,150,150, 150, 150), learning_rate_init = .01)\n",
    "#     return MLPClassifier(hidden_layer_sizes=(200,200,200), learning_rate_init = .01)\n",
    "#    return MLPClassifier(hidden_layer_sizes=(100,100,100), learning_rate_init = .01)\n",
    "#    return MLPClassifier(hidden_layer_sizes=(90,80,70,60), learning_rate_init = .01)\n",
    "#     return MLPClassifier(hidden_layer_sizes=(100,70,60,30,60,70,100), learning_rate_init = .002)\n",
    "#     return MLPClassifier(hidden_layer_sizes=(100,70,60,30), learning_rate_init = .002)\n",
    "#     return MLPClassifier(hidden_layer_sizes=(150,100,50,30), learning_rate_init = .001)\n",
    "#     return MLPClassifier(hidden_layer_sizes=(150,120,90,60,30), learning_rate_init = .001)\n",
    "#     return MLPClassifier(hidden_layer_sizes=(150,130,110,90,70,50), learning_rate_init = .001)\n",
    "#     return MLPClassifier(hidden_layer_sizes=(150,130,110,90,75,60,40), learning_rate_init = .001)\n",
    "#     return MLPClassifier(hidden_layer_sizes=(250,200,150,130,110,90), learning_rate_init = .0003)\n",
    "#     return MLPClassifier(hidden_layer_sizes=(300,250,200,170,140,110,90,50), learning_rate_init = .0003)\n",
    "#     return MLPClassifier(hidden_layer_sizes=(500,400,300,250,200,160,120,90,50), learning_rate_init = .0006)\n",
    "#    return MLPClassifier(hidden_layer_sizes=(80,80,80), learning_rate_init = .01)\n",
    "    return MLPClassifier(hidden_layer_sizes=(30,30,30,30,30,30,30,30,30,30), learning_rate_init = .0376)\n",
    "\n",
    "def _lr():\n",
    "    return linear_model.LogisticRegression(solver='lbfgs', class_weight={0:1,1:2})\n",
    "\n",
    "def _knn():\n",
    "    return KNeighborsClassifier(n_neighbors=8, weights='distance')\n",
    "\n",
    "def _gb():\n",
    "    return GradientBoostingClassifier(learning_rate = .31, max_depth = 3)\n",
    "\n",
    "def _ensemble():\n",
    "    estimators = [\n",
    "        ('LR', _lr()),\n",
    "        ('SVM', _svm()),\n",
    "        ('KNN', _knn()),\n",
    "        ('MLP', _mlp()),\n",
    "        ('RF', _rf()),\n",
    "        ('GB', _gb()),\n",
    "#         ('NB', _nb())\n",
    "    ]\n",
    "#     estimators = scaled_ensemble()\n",
    "    \n",
    "    return VotingClassifier(estimators, voting='hard')\n",
    "\n",
    "def make_classifier(classifier) :\n",
    "    if classifier == 'mlp':\n",
    "        return _mlp()\n",
    "    elif classifier == 'rf':\n",
    "        return _rf()\n",
    "    elif classifier == 'nb':\n",
    "        return _nb()\n",
    "    elif classifier == 'knn':\n",
    "        return _knn()\n",
    "    elif classifier == 'svm':\n",
    "        return _svm()\n",
    "    elif classifier == 'lr':\n",
    "        return _lr()\n",
    "    elif classifier == 'gb':\n",
    "        return _gb()\n",
    "    elif classifier == 'ensemble':\n",
    "        return _ensemble()\n",
    "    else:\n",
    "        raise ValueError('Not a correct key for classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_ensemble():\n",
    "    return [\n",
    "        ('MLP_500', MLPClassifier(hidden_layer_sizes=(500,400,300,250,200,160,120,90,50), learning_rate_init = .0006)),\n",
    "        ('MLP_300', MLPClassifier(hidden_layer_sizes=(300,250,200,170,140,110,90,50), learning_rate_init = .0003)),\n",
    "        ('MLP_250', MLPClassifier(hidden_layer_sizes=(250,200,150,130,110,90), learning_rate_init = .0003)),\n",
    "        ('MLP_150', MLPClassifier(hidden_layer_sizes=(150,100,50,30), learning_rate_init = .001)),\n",
    "        ('MLP_30', MLPClassifier(hidden_layer_sizes=(30,30,30,30,30,30,30,30,30,30), learning_rate_init = .0376)),\n",
    "        ('svm_orig', svm.SVC(probability=True)),\n",
    "        ('svm_linear', svm.SVC(kernel='linear', probability=True, class_weight={0:25,1:1})),\n",
    "        ('svm_nusvc_linear', svm.NuSVC(kernel='linear', probability=True, class_weight=\"balanced\")),\n",
    "        ('svm_nusvc_rbf', svm.NuSVC(kernel='rbf', probability=True, class_weight=\"balanced\")),\n",
    "        ('logreg_25_1', linear_model.LogisticRegression(solver='lbfgs', class_weight={0:25,1:1})),\n",
    "        ('logreg_blcd', linear_model.LogisticRegression(solver='lbfgs', class_weight=\"balanced\")),\n",
    "        ('rf_norm', RandomForestClassifier(n_estimators=25,\n",
    "                                max_depth=9,\n",
    "                                min_samples_split=2,\n",
    "                                min_samples_leaf=1,\n",
    "                                min_weight_fraction_leaf=0,\n",
    "                                max_leaf_nodes=25,\n",
    "                                bootstrap=False,\n",
    "                                random_state=0)),\n",
    "        ('rf_blcd_25_9', RandomForestClassifier(n_estimators=25,\n",
    "                                max_depth=9,\n",
    "                                min_samples_split=2,\n",
    "                                min_samples_leaf=1,\n",
    "                                min_weight_fraction_leaf=0,\n",
    "                                max_leaf_nodes=25,\n",
    "                                bootstrap=False,\n",
    "                                random_state=0)),\n",
    "        ('rf_blcd_100_9', RandomForestClassifier(n_estimators=100,\n",
    "                                max_depth=9,\n",
    "                                min_samples_split=2,\n",
    "                                min_samples_leaf=1,\n",
    "                                min_weight_fraction_leaf=0,\n",
    "                                max_leaf_nodes=25,\n",
    "                                bootstrap=False,\n",
    "                                random_state=0,\n",
    "                                class_weight=\"balanced\")),\n",
    "        ('rf_blcd_100_15', RandomForestClassifier(n_estimators=100,\n",
    "                                max_depth=15,\n",
    "                                min_samples_split=2,\n",
    "                                min_samples_leaf=1,\n",
    "                                min_weight_fraction_leaf=0,\n",
    "                                max_leaf_nodes=25,\n",
    "                                bootstrap=False,\n",
    "                                random_state=0,\n",
    "                                class_weight=\"balanced\")),\n",
    "        ('rf_blcd_150_15', RandomForestClassifier(n_estimators=50,\n",
    "                                max_depth=15,\n",
    "                                min_samples_split=2,\n",
    "                                min_samples_leaf=1,\n",
    "                                min_weight_fraction_leaf=0,\n",
    "                                max_leaf_nodes=25,\n",
    "                                bootstrap=False,\n",
    "                                random_state=0,\n",
    "                                class_weight=\"balanced\")),\n",
    "        ('knn_8', KNeighborsClassifier(n_neighbors=8, weights='distance')),\n",
    "        ('knn_12', KNeighborsClassifier(n_neighbors=12, weights='distance')),\n",
    "        ('knn_10', KNeighborsClassifier(n_neighbors=10, weights='distance')),\n",
    "        ('gb_31_3', GradientBoostingClassifier(learning_rate = .31, max_depth = 3)),\n",
    "        ('gb_15_5', GradientBoostingClassifier(learning_rate = .15, max_depth = 5)),\n",
    "        ('gb_07_6', GradientBoostingClassifier(learning_rate = .07, max_depth = 6))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Training orig_scan/trainingReformatedCamda_PC3.txt.gz with ensemble\n",
      "Iteration 2: Training orig_scan/trainingReformatedCamda_MCF7.txt.gz with ensemble\n",
      "Iteration 3: Testing orig_scan/testReformatedCamda_PC3.txt.gz with ensemble\n",
      "Iteration 4: Testing orig_scan/testReformatedCamda_MCF7.txt.gz with ensemble\n",
      "done\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Test/Train</th>\n",
       "      <th>Cell Line</th>\n",
       "      <th>Data_Version</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ensemble</td>\n",
       "      <td>Training</td>\n",
       "      <td>PC3</td>\n",
       "      <td>orig_scan</td>\n",
       "      <td>0.647368</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ensemble</td>\n",
       "      <td>Training</td>\n",
       "      <td>MCF7</td>\n",
       "      <td>orig_scan</td>\n",
       "      <td>0.668421</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.095002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ensemble</td>\n",
       "      <td>Test</td>\n",
       "      <td>PC3</td>\n",
       "      <td>orig_scan</td>\n",
       "      <td>0.744186</td>\n",
       "      <td>0.940299</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>-0.012534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ensemble</td>\n",
       "      <td>Test</td>\n",
       "      <td>MCF7</td>\n",
       "      <td>orig_scan</td>\n",
       "      <td>0.709302</td>\n",
       "      <td>0.865672</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.028216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Method Test/Train Cell Line Data_Version  Accuracy  Sensitivity  \\\n",
       "1  ensemble   Training       PC3    orig_scan  0.647368     0.900000   \n",
       "2  ensemble   Training      MCF7    orig_scan  0.668421     0.900000   \n",
       "3  ensemble       Test       PC3    orig_scan  0.744186     0.940299   \n",
       "4  ensemble       Test      MCF7    orig_scan  0.709302     0.865672   \n",
       "\n",
       "   Specificity       MCC  \n",
       "1     0.100000  0.000000  \n",
       "2     0.166667  0.095002  \n",
       "3     0.052632 -0.012534  \n",
       "4     0.157895  0.028216  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifiers = [\n",
    "#     'rf',\n",
    "#     'svm',\n",
    "#     'nb',\n",
    "#     'mlp',\n",
    "#     'knn',\n",
    "#     'lr',\n",
    "#     'gb',\n",
    "    'ensemble'\n",
    "]\n",
    "train_files = [\n",
    "    [orig_scan_pc3_train, 'Training', 'PC3', 'orig_scan'],\n",
    "    [orig_scan_mcf7_train, 'Training', 'MCF7', 'orig_scan'],\n",
    "#     [scan_pc3_train, 'Training', 'PC3', 'updated_scan'],\n",
    "#     [scan_mcf7_train, 'Training', 'MCF7', 'updated_scan'],\n",
    "#     [farms_pc3_train, 'Training', 'PC3', 'farms'],\n",
    "#     [farms_mcf7_train, 'Training', 'MCF7', 'farms']\n",
    "]\n",
    "test_files = [\n",
    "    [orig_scan_pc3_train, orig_scan_pc3_test, 'Test', 'PC3', 'orig_scan'],\n",
    "    [orig_scan_mcf7_train, orig_scan_mcf7_test, 'Test', 'MCF7', 'orig_scan'],\n",
    "#     [scan_pc3_train, scan_pc3_test, 'Test', 'PC3', 'updated_scan'],\n",
    "#     [scan_mcf7_train, scan_mcf7_test, 'Test', 'MCF7', 'updated_scan'],\n",
    "#     [farms_pc3_train, farms_pc3_test, 'Test', 'PC3', 'farms'],\n",
    "#     [farms_mcf7_train, farms_mcf7_test, 'Test', 'MCF7', 'farms'],\n",
    "]\n",
    "\n",
    "col_names =  ['Method', 'Test/Train', 'Cell Line', 'Data_Version', 'Accuracy', 'Sensitivity', 'Specificity', 'MCC']\n",
    "df  = pd.DataFrame(columns = col_names)\n",
    "\n",
    "i = 0\n",
    "for classifier in classifiers:\n",
    "    for file_set in train_files:\n",
    "        i += 1\n",
    "        print(\"Iteration {}: Training {} with {}\".format(i, file_set[0], classifier))\n",
    "        y_test, predictions, _ = train(file_set[0],classifier)\n",
    "        df.loc[i] = [classifier, file_set[1], file_set[2], file_set[3]] + get_Results(y_test, predictions)\n",
    "        \n",
    "for classifier in classifiers:\n",
    "    for file_set in test_files:\n",
    "        i += 1\n",
    "        print(\"Iteration {}: Testing {} with {}\".format(i, file_set[1], classifier))\n",
    "        y_test, predictions, _ = test(file_set[0], file_set[1], classifier)\n",
    "        df.loc[i] = [classifier, file_set[2], file_set[3], file_set[4]] + get_Results(y_test, predictions)\n",
    "        \n",
    "print('done')\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_df = df\n",
    "# original_df.to_csv('original.csv')\n",
    "\n",
    "# updated_svm_df = df\n",
    "# updated_svm_df.to_csv('updated_svm.csv')\n",
    "# original_df\n",
    "\n",
    "# robust_scaler_df = df\n",
    "# robust_scaler_df.to_csv('robust_scaler.csv')\n",
    "\n",
    "# orig_anova_df = df\n",
    "# orig_anova_df.to_csv('orig_anova.csv')\n",
    "\n",
    "# orig_pca_df = df\n",
    "# orig_pca_df.to_csv('orig_pca.csv')\n",
    "\n",
    "# balanced_df = df\n",
    "# balanced_df.to_csv('balanced.csv')\n",
    "\n",
    "# weight_1_50_df = df\n",
    "# weight_1_50_df.to_csv('weight_1_50.csv')\n",
    "\n",
    "# weight_50_1_df = df\n",
    "# weight_50_1_df.to_csv('weight_50_1.csv')\n",
    "\n",
    "# weight_25_1_df = df\n",
    "# weight_25_1_df.to_csv('weight_25_1.csv')\n",
    "\n",
    "# weight_10_1_df = df\n",
    "# weight_10_1_df.to_csv('weight_10_1.csv')\n",
    "\n",
    "# weight_5_1_df = df\n",
    "# weight_5_1_df.to_csv('weight_5_1.csv')\n",
    "\n",
    "# weight_2_1_df = df\n",
    "# weight_2_1_df.to_csv('weight_2_1.csv')\n",
    "\n",
    "# weight_1_2_df = df\n",
    "# weight_1_2_df.to_csv('weight_1_2.csv')\n",
    "\n",
    "# scaled_ensemble_df = df\n",
    "# scaled_ensemble_df.to_csv('scaled_ensemble.csv')\n",
    "\n",
    "# scaled_ensemble_hard_df = df\n",
    "# scaled_ensemble_hard_df.to_csv('scaled_ensemble_hard.csv')\n",
    "\n",
    "norm_ensemble_hard_df = df\n",
    "norm_ensemble_hard_df.to_csv('norm_ensemble_hard.csv')\n",
    "\n",
    "# original_df = pd.read_csv('original.csv')\n",
    "# updated_svm_df = pd.read_csv('updated_svm.csv')\n",
    "# robust_scaler_df = pd.read_csv('robust_scaler.csv')\n",
    "# orig_anova_df = pd.read_csv('orig_anova.csv')\n",
    "# orig_pca_df = pd.read_csv('orig_pca.csv')\n",
    "# scaled_ensemble_df = pd.read_csv('scaled_ensemble.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# origClassifiers_farms = original_df[original_df['Data_Version'] == 'farms']\n",
    "# origClassifiers_orig_scan = original_df[original_df['Data_Version'] == 'orig_scan']\n",
    "# origClassifiers_updated_scan = original_df[original_df['Data_Version'] == 'updated_scan']\n",
    "\n",
    "# limited_orig_scan = origClassifiers_orig_scan.loc[:,['Accuracy', 'Sensitivity', 'Specificity', 'MCC']]\n",
    "# limited_orig_scan.index = range(1,33)\n",
    "\n",
    "# limited_farms = origClassifiers_farms.loc[:,['Accuracy', 'Sensitivity', 'Specificity', 'MCC']]\n",
    "# limited_farms.index = range(1,33)\n",
    "\n",
    "# limited_updated_scan = origClassifiers_updated_scan.loc[:,['Accuracy', 'Sensitivity', 'Specificity', 'MCC']]\n",
    "# limited_updated_scan.index = range(1,33)\n",
    "\n",
    "# limited_robust_scaler_df = robust_scaler_df.loc[:,['Accuracy', 'Sensitivity', 'Specificity', 'MCC']]\n",
    "# limited_robust_scaler_df.index = range(1,33)\n",
    "\n",
    "# limited_orig_anova_df = orig_anova_df.loc[:,['Accuracy', 'Sensitivity', 'Specificity', 'MCC']]\n",
    "# limited_orig_anova_df.index = range(1,33)\n",
    "\n",
    "# limited_orig_pca_df = orig_pca_df.loc[:,['Accuracy', 'Sensitivity', 'Specificity', 'MCC']]\n",
    "# limited_orig_pca_df.index = range(1,33)\n",
    "\n",
    "# limited_balanced_df = balanced_df.loc[:,['Accuracy', 'Sensitivity', 'Specificity', 'MCC']]\n",
    "# limited_balanced_df.index = range(1,33)\n",
    "\n",
    "# # limited_weight_1_50_df = weight_1_50_df.loc[:,['Accuracy', 'Sensitivity', 'Specificity', 'MCC']]\n",
    "# # limited_weight_1_50_df.index = range(1,33)\n",
    "\n",
    "# limited_weight_50_1_df = weight_50_1_df.loc[:,['Accuracy', 'Sensitivity', 'Specificity', 'MCC']]\n",
    "# limited_weight_50_1_df.index = range(1,33)\n",
    "\n",
    "# limited_weight_25_1_df = weight_25_1_df.loc[:,['Accuracy', 'Sensitivity', 'Specificity', 'MCC']]\n",
    "# limited_weight_25_1_df.index = range(1,33)\n",
    "\n",
    "# limited_weight_10_1_df = weight_10_1_df.loc[:,['Accuracy', 'Sensitivity', 'Specificity', 'MCC']]\n",
    "# limited_weight_10_1_df.index = range(1,33)\n",
    "\n",
    "# limited_weight_5_1_df = weight_5_1_df.loc[:,['Accuracy', 'Sensitivity', 'Specificity', 'MCC']]\n",
    "# limited_weight_5_1_df.index = range(1,33)\n",
    "\n",
    "# limited_weight_2_1_df = weight_2_1_df.loc[:,['Accuracy', 'Sensitivity', 'Specificity', 'MCC']]\n",
    "# limited_weight_2_1_df.index = range(1,33)\n",
    "\n",
    "# limited_weight_1_2_df = weight_1_2_df.loc[:,['Accuracy', 'Sensitivity', 'Specificity', 'MCC']]\n",
    "# limited_weight_1_2_df.index = range(1,33)\n",
    "\n",
    "# limited_scaled_ensemble_df = scaled_ensemble_df.loc[:,['Accuracy', 'Sensitivity', 'Specificity', 'MCC']]\n",
    "\n",
    "# limited_scaled_ensemble_hard_df = scaled_ensemble_hard_df.loc[:,['Accuracy', 'Sensitivity', 'Specificity', 'MCC']]\n",
    "\n",
    "limited_norm_ensemble_hard_df = norm_ensemble_hard_df.loc[:,['Accuracy', 'Sensitivity', 'Specificity', 'MCC']]\n",
    "\n",
    "# limited_df = df.loc[:,['Accuracy', 'Sensitivity', 'Specificity', 'MCC']]\n",
    "# limited_df.index = range(1,33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>farms</th>\n",
       "      <td>0.004682</td>\n",
       "      <td>0.011291</td>\n",
       "      <td>-0.027741</td>\n",
       "      <td>-0.017493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>updated_scan</th>\n",
       "      <td>0.017044</td>\n",
       "      <td>0.036101</td>\n",
       "      <td>-0.042105</td>\n",
       "      <td>-0.008123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Accuracy  Sensitivity  Specificity       MCC\n",
       "farms         0.004682     0.011291    -0.027741 -0.017493\n",
       "updated_scan  0.017044     0.036101    -0.042105 -0.008123"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_names =  ['Accuracy', 'Sensitivity', 'Specificity', 'MCC']\n",
    "data_preprocessing  = pd.DataFrame(columns = col_names)\n",
    "\n",
    "data_preprocessing.loc['farms'] = limited_farms.sub(limited_orig_scan).mean()\n",
    "data_preprocessing.loc['updated_scan'] = limited_updated_scan.sub(limited_orig_scan).mean()\n",
    "\n",
    "data_preprocessing\n",
    "#limited_farms.sub(limited_updated_scan).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>robust_scaler</th>\n",
       "      <td>-0.015338</td>\n",
       "      <td>-0.042900</td>\n",
       "      <td>0.047122</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anova_f_stat</th>\n",
       "      <td>-0.016547</td>\n",
       "      <td>-0.048777</td>\n",
       "      <td>0.064611</td>\n",
       "      <td>0.021450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pca</th>\n",
       "      <td>0.001771</td>\n",
       "      <td>-0.002777</td>\n",
       "      <td>0.004030</td>\n",
       "      <td>-0.008689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Accuracy  Sensitivity  Specificity       MCC\n",
       "robust_scaler -0.015338    -0.042900     0.047122  0.000900\n",
       "anova_f_stat  -0.016547    -0.048777     0.064611  0.021450\n",
       "pca            0.001771    -0.002777     0.004030 -0.008689"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_names =  ['Accuracy', 'Sensitivity', 'Specificity', 'MCC']\n",
    "feature_selection  = pd.DataFrame(columns = col_names)\n",
    "\n",
    "feature_selection.loc['robust_scaler'] = limited_robust_scaler_df.sub(limited_orig_scan).mean()\n",
    "feature_selection.loc['anova_f_stat'] = limited_orig_anova_df.sub(limited_orig_scan).mean()\n",
    "feature_selection.loc['pca'] = limited_orig_pca_df.sub(limited_orig_scan).mean()\n",
    "\n",
    "feature_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>weight_50_1</th>\n",
       "      <td>-0.077769</td>\n",
       "      <td>-0.169984</td>\n",
       "      <td>0.171738</td>\n",
       "      <td>-0.038169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight_25_1</th>\n",
       "      <td>-0.095972</td>\n",
       "      <td>-0.202224</td>\n",
       "      <td>0.199973</td>\n",
       "      <td>-0.033314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight_10_1</th>\n",
       "      <td>-0.059165</td>\n",
       "      <td>-0.129653</td>\n",
       "      <td>0.125822</td>\n",
       "      <td>-0.035778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight_5_1</th>\n",
       "      <td>-0.041149</td>\n",
       "      <td>-0.093251</td>\n",
       "      <td>0.094518</td>\n",
       "      <td>-0.033954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight_2_1</th>\n",
       "      <td>-0.033300</td>\n",
       "      <td>-0.079976</td>\n",
       "      <td>0.088788</td>\n",
       "      <td>-0.027315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>balanced</th>\n",
       "      <td>-0.012515</td>\n",
       "      <td>-0.036833</td>\n",
       "      <td>0.043476</td>\n",
       "      <td>-0.002045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight_1_2</th>\n",
       "      <td>-0.025402</td>\n",
       "      <td>-0.060810</td>\n",
       "      <td>0.060033</td>\n",
       "      <td>-0.033553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Accuracy  Sensitivity  Specificity       MCC\n",
       "weight_50_1 -0.077769    -0.169984     0.171738 -0.038169\n",
       "weight_25_1 -0.095972    -0.202224     0.199973 -0.033314\n",
       "weight_10_1 -0.059165    -0.129653     0.125822 -0.035778\n",
       "weight_5_1  -0.041149    -0.093251     0.094518 -0.033954\n",
       "weight_2_1  -0.033300    -0.079976     0.088788 -0.027315\n",
       "balanced    -0.012515    -0.036833     0.043476 -0.002045\n",
       "weight_1_2  -0.025402    -0.060810     0.060033 -0.033553"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_names =  ['Accuracy', 'Sensitivity', 'Specificity', 'MCC']\n",
    "class_weights  = pd.DataFrame(columns = col_names)\n",
    "\n",
    "class_weights.loc['weight_50_1'] = limited_weight_50_1_df.sub(limited_orig_scan).mean()\n",
    "class_weights.loc['weight_25_1'] = limited_weight_25_1_df.sub(limited_orig_scan).mean()\n",
    "class_weights.loc['weight_10_1'] = limited_weight_10_1_df.sub(limited_orig_scan).mean()\n",
    "class_weights.loc['weight_5_1'] = limited_weight_5_1_df.sub(limited_orig_scan).mean()\n",
    "class_weights.loc['weight_2_1'] = limited_weight_2_1_df.sub(limited_orig_scan).mean()\n",
    "class_weights.loc['balanced'] = limited_balanced_df.sub(limited_orig_scan).mean()\n",
    "class_weights.loc['weight_1_2'] = limited_weight_1_2_df.sub(limited_orig_scan).mean()\n",
    "# class_weights.loc['weight_1_2'] = limited_weight_1_2_df.sub(limited_orig_scan).mean()\n",
    "# class_weights.loc['pca'] = limited_orig_pca_df.sub(limited_orig_scan).mean()\n",
    "\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>scaled_soft</th>\n",
       "      <td>0.000612</td>\n",
       "      <td>-0.082319</td>\n",
       "      <td>0.139474</td>\n",
       "      <td>0.040147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scaled_hard</th>\n",
       "      <td>-0.025887</td>\n",
       "      <td>-0.067853</td>\n",
       "      <td>0.094298</td>\n",
       "      <td>0.019389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>norm_hard</th>\n",
       "      <td>-0.027968</td>\n",
       "      <td>-0.062543</td>\n",
       "      <td>0.063816</td>\n",
       "      <td>-0.016690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Accuracy  Sensitivity  Specificity       MCC\n",
       "scaled_soft  0.000612    -0.082319     0.139474  0.040147\n",
       "scaled_hard -0.025887    -0.067853     0.094298  0.019389\n",
       "norm_hard   -0.027968    -0.062543     0.063816 -0.016690"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limited_ensemble_orig_scan = limited_orig_scan.loc[15:16].append(limited_orig_scan.loc[31:32])\n",
    "limited_ensemble_orig_scan.index = range(1,5)\n",
    "\n",
    "col_names =  ['Accuracy', 'Sensitivity', 'Specificity', 'MCC']\n",
    "class_weights  = pd.DataFrame(columns = col_names)\n",
    "\n",
    "class_weights.loc['scaled_soft'] = limited_scaled_ensemble_df.sub(limited_ensemble_orig_scan).mean()\n",
    "class_weights.loc['scaled_hard'] = limited_scaled_ensemble_hard_df.sub(limited_ensemble_orig_scan).mean()\n",
    "class_weights.loc['norm_hard'] = limited_norm_ensemble_hard_df.sub(limited_ensemble_orig_scan).mean()\n",
    "\n",
    "class_weights\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp = tmp_orig_pca_df.sub(tmp_orig_scan).mean()\n",
    "# tmp_df = tmp_orig_pca_df ## pca 10\n",
    "# tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
